{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from xai.drise_batch import DRISEBatch\n",
    "from utils.utils import load_and_convert_bboxes\n",
    "from utils.plot_utils import plot_image_with_bboxes, plot_saliency_and_targetbb_on_image\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        \n",
    "args = Args(**{\n",
    "    'img_name': '00901',\n",
    "    'model_path': 'use_case/models/best.pt',\n",
    "    'datadir': 'use_case/',\n",
    "    'annotations_dir': 'use_case/',\n",
    "    'device': 'cuda:0',\n",
    "    'input_size': (480, 640),\n",
    "    'gpu_batch': 16,\n",
    "    'mask_type': 'rise',\n",
    "    'maskdir': 'masks/',\n",
    "    'N': 1000,\n",
    "    'resolution': 8,\n",
    "    'p1': 0.5,\n",
    "    'target_classes': [0],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Import data\n",
    "#########################\n",
    "height, width = args.input_size\n",
    "img_path = args.datadir + args.img_name\n",
    "orig_img = Image.open(img_path + '.jpg')\n",
    "resized_img = orig_img.resize((width, height), Image.LANCZOS)\n",
    "img_np = np.array(resized_img)\n",
    "\n",
    "plt.imshow(img_np)\n",
    "print(img_np.shape)\n",
    "print(img_np.shape, img_np.dtype)\n",
    "\n",
    "# preprocessing function\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "tensor = preprocess(img_np)\n",
    "tensor = tensor.unsqueeze(0).to(args.device) # 1,3,224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "labels = args.annotations_dir + args.img_name + '.txt'\n",
    "print('Labels directory:', labels)\n",
    "\n",
    "date_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "for tc in [0]:#[0,1,2,3,4,5,6,7]:\n",
    "    bboxes, _ = load_and_convert_bboxes(labels,img_height=args.input_size[0],img_width=args.input_size[1], target_class= tc)\n",
    "    plot_image_with_bboxes(img_np,bboxes, save_to=f'output/{args.img_name}_bboxes_class{tc}_{date_time}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = args.target_classes[0] # select a single class (list is given)\n",
    "bboxes, _ = load_and_convert_bboxes(labels,img_height=args.input_size[0],img_width=args.input_size[1], target_class= target_class)\n",
    "target_bbox = bboxes[0] # select the first bbox --> multiple might be given in the same image\n",
    "plot_image_with_bboxes(img_np,[target_bbox], save_to=f'output/{args.img_name}_target_bbox_class{target_class}_{date_time}.png')\n",
    "print('Target bbox:',target_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO\n",
    "Load model and test to see its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(args.model_path, task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model.predict(tensor) \n",
    "\n",
    "boxes = results[0].boxes  # Assuming we have one image and accessing the first result\n",
    "predicted_bboxes = []\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "    # Convert to a list of (x, y) tuples\n",
    "    bbox = [(x1, y1), (x2, y1), (x2, y2), (x1, y2)]\n",
    "    predicted_bboxes.append(bbox)\n",
    "# print(bboxes)\n",
    "image_with_bboxes = plot_image_with_bboxes(img_np, predicted_bboxes, save_to=f'output/{args.img_name}_predicted_bboxes_{date_time}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-RISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = DRISEBatch(\n",
    "    model=model, \n",
    "    input_size=args.input_size, \n",
    "    device=args.device,\n",
    "    gpu_batch=args.gpu_batch\n",
    ")\n",
    "\n",
    "# if generate or load\n",
    "generate_new = True\n",
    "\n",
    "mask_filename = f'{args.mask_type}_n{args.N}_s{args.resolution}_p{args.p1}_{args.input_size[0]}x{args.input_size[1]}'\n",
    "mask_path = args.maskdir + mask_filename + '.npy'\n",
    "print(mask_path)\n",
    "\n",
    "if generate_new or not os.path.isfile(mask_path):\n",
    "    explainer.generate_masks_rise(N=args.N, s=args.resolution, p1=args.p1, savepath= mask_path)\n",
    "else:\n",
    "    explainer.load_masks(mask_path)\n",
    "    print('Masks are loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize 3 generated masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_masks=3\n",
    "masks = explainer.masks[:3]\n",
    "\n",
    "masked_image = torch.mul(masks.to(args.device), tensor)\n",
    "masked_image = masked_image.cpu().numpy()\n",
    "print(masked_image.shape)\n",
    "\n",
    "masks = masks.cpu()\n",
    "if masks.ndim == 4:  # If the masks have a shape of (N, 1, H, W)\n",
    "    masks = masks[:, 0, :, :]\n",
    "\n",
    "fig, axes = plt.subplots(3, num_masks, figsize=(15, 10))\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    ax.imshow(img_np)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Original image {i + 1}')\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    ax.imshow(masks[i], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Mask {i + 1}')\n",
    "\n",
    "for i, ax in enumerate(axes[2]):\n",
    "    ax.imshow(masked_image[i].transpose(1, 2, 0))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Masked Image {i + 1}')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tensor.permute(0,1,3,2).shape)\n",
    "saliency = explainer(\n",
    "    x=tensor,\n",
    "    target_class_indices=args.target_classes,\n",
    "    target_bbox=target_bbox\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot heatmap/saliency map given by XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_saliency_and_targetbb_on_image(\n",
    "        height=args.input_size[0], width=args.input_size[1], \n",
    "        img_name=args.img_name, \n",
    "        img=img_np,\n",
    "        saliency_map=saliency[target_class], \n",
    "        target_class_id= target_class,\n",
    "        target_bbox=target_bbox,\n",
    "        save_to=f'output/{args.img_name}_saliency_targetbb_class{target_class}_{date_time}.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do the same for another target bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes, _ = load_and_convert_bboxes(labels,img_height=args.input_size[0],img_width=args.input_size[1], target_class= target_class)\n",
    "target_bbox = bboxes[1] # select the first bbox --> multiple might be given in the same image\n",
    "plot_image_with_bboxes(img_np,[target_bbox])\n",
    "print('Target bbox:',target_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency = explainer(x=tensor,\n",
    "                     target_class_indices=[target_class],\n",
    "                     target_bbox=target_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_bbox_and_saliency = plot_saliency_and_targetbb_on_image(\n",
    "        height=args.input_size[0], width=args.input_size[1], \n",
    "        img_name=args.img_name, \n",
    "        img=img_np,\n",
    "        saliency_map=saliency[target_class], \n",
    "        target_class_id= target_class,\n",
    "        target_bbox=target_bbox\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# what images to send to llama: \n",
    "# orig. image\n",
    "resized_img\n",
    "#yolo detected bboxes\n",
    "\n",
    "yoloPredictedBboxes = Image.open(f'output/{args.img_name}_predicted_bboxes_{date_time}.png')\n",
    "\n",
    "# saliency map with target bbox\n",
    "\n",
    "driseSaliency = Image.open(f'output/{args.img_name}_saliency_targetbb_class{target_class}_{date_time}.png')\n",
    "\n",
    "\n",
    "images = [resized_img, yoloPredictedBboxes, driseSaliency]\n",
    "widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "total_width = sum(widths)\n",
    "max_height = max(heights)\n",
    "\n",
    "composed = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "x_offset = 0\n",
    "for im in images:\n",
    "  composed.paste(im, (x_offset,0))\n",
    "  x_offset += im.size[0]\n",
    "\n",
    "composed.save(f'output/composedImage{target_class}_{date_time}.jpg')\n",
    "\n",
    "# url = \"https://s7ap1.scene7.com/is/image/destqueensland/teq/consumer/global/images/destinations/sunshine-coast/blog-images/editorial-hero-banner/2018_SC_Koala_Wildlife_Australiazoo.jpg?bfc=on&fit=wrap&fmt=webp&qlt=100&resMode=bisharp&wid=1200\"\n",
    "\n",
    "# response = requests.get(url)\n",
    "# image = Image.open(BytesIO(response.content))\n",
    "\n",
    "instruction = \"\"\"You are the Visual LLM specializing in detailed explanation chaining for Visual-LLMs. \n",
    "When provided with an image that includes an object, its corresponding bounding box, and a saliency map, your task is to deliver a comprehensive and in-depth analysis of the selected areas within the bounding box. \n",
    "\n",
    "Focus on unpacking the rationale behind the model's attention to these highlighted regions. Your response should encompass an extensive discussion on several key features influencing the model's decisions, including but not limited to:\n",
    "\n",
    "- **Color:** Analyze how the colors present may hold significance in drawing model attention.\n",
    "- **Texture:** Discuss the textural elements and how they contribute to the understanding of the object and its environment.\n",
    "- **Shape:** Examine the geometric attributes of the object and how they interact with the bounding box.\n",
    "- **Context:** Consider the contextual elements of the image that may provide additional meaning or relevance to the selected focal areas.\n",
    "\n",
    "Your explanation should aim for clarity, depth, and informativeness, ensuring that every aspect of the analysis is well-articulated and beneficial for the user's understanding.\n",
    "\"\"\"\n",
    "\n",
    "# print(type(resized_img), type(yoloPredictedBboxes), type(driseSaliency))\n",
    "\n",
    "# # Process images properly\n",
    "# image_inputs = [resized_img, yoloPredictedBboxes, driseSaliency]\n",
    "\n",
    "# image_inputs = [img.convert(\"RGB\").resize((224,224)) for img in image_inputs]\n",
    "\n",
    "# for img in image_inputs:\n",
    "#     print(img, img.size)\n",
    "\n",
    "# # Concatenate images horizontally\n",
    "# total_width = sum(img.width for img in image_inputs)\n",
    "# max_height = max(img.height for img in image_inputs)\n",
    "\n",
    "# composed = Image.new(\"RGB\", (total_width, max_height))\n",
    "# x_offset = 0\n",
    "\n",
    "\n",
    "# for img in image_inputs:\n",
    "#     composed.paste(img, (x_offset, 0))\n",
    "#     x_offset += img.width\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    composed,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 400,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from xai.drise_batch import DRISEBatch\n",
    "from utils.utils import load_and_convert_bboxes\n",
    "from utils.plot_utils import plot_image_with_bboxes, plot_saliency_and_targetbb_on_image\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        \n",
    "args = Args(**{\n",
    "    'img_name': '00901',\n",
    "    'model_path': 'use_case/models/best.pt',\n",
    "    'datadir': 'use_case/',\n",
    "    'annotations_dir': 'use_case/',\n",
    "    'device': 'cuda:0',\n",
    "    'input_size': (480, 640),\n",
    "    'gpu_batch': 16,\n",
    "    'mask_type': 'rise',\n",
    "    'maskdir': 'masks/',\n",
    "    'N': 1000,\n",
    "    'resolution': 8,\n",
    "    'p1': 0.5,\n",
    "    'target_classes': [0],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Import data\n",
    "#########################\n",
    "height, width = args.input_size\n",
    "img_path = args.datadir + args.img_name\n",
    "orig_img = Image.open(img_path + '.jpg')\n",
    "resized_img = orig_img.resize((width, height), Image.LANCZOS)\n",
    "img_np = np.array(resized_img)\n",
    "\n",
    "plt.imshow(img_np)\n",
    "print(img_np.shape)\n",
    "print(img_np.shape, img_np.dtype)\n",
    "\n",
    "# preprocessing function\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "tensor = preprocess(img_np)\n",
    "tensor = tensor.unsqueeze(0).to(args.device) # 1,3,224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "labels = args.annotations_dir + args.img_name + '.txt'\n",
    "print('Labels directory:', labels)\n",
    "\n",
    "date_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "for tc in [0]:#[0,1,2,3,4,5,6,7]:\n",
    "    bboxes, _ = load_and_convert_bboxes(labels,img_height=args.input_size[0],img_width=args.input_size[1], target_class= tc)\n",
    "    plot_image_with_bboxes(img_np,bboxes, save_to=f'output/{args.img_name}_bboxes_class{tc}_{date_time}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = args.target_classes[0] # select a single class (list is given)\n",
    "bboxes, _ = load_and_convert_bboxes(labels,img_height=args.input_size[0],img_width=args.input_size[1], target_class= target_class)\n",
    "target_bbox = bboxes[0] # select the first bbox --> multiple might be given in the same image\n",
    "plot_image_with_bboxes(img_np,[target_bbox], save_to=f'output/{args.img_name}_target_bbox_class{target_class}_{date_time}.png')\n",
    "print('Target bbox:',target_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO\n",
    "Load model and test to see its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(args.model_path, task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model.predict(tensor) \n",
    "\n",
    "boxes = results[0].boxes  # Assuming we have one image and accessing the first result\n",
    "predicted_bboxes = []\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "    # Convert to a list of (x, y) tuples\n",
    "    bbox = [(x1, y1), (x2, y1), (x2, y2), (x1, y2)]\n",
    "    predicted_bboxes.append(bbox)\n",
    "# print(bboxes)\n",
    "image_with_bboxes = plot_image_with_bboxes(img_np, predicted_bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-RISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = DRISEBatch(\n",
    "    model=model, \n",
    "    input_size=args.input_size, \n",
    "    device=args.device,\n",
    "    gpu_batch=args.gpu_batch\n",
    ")\n",
    "\n",
    "# if generate or load\n",
    "generate_new = True\n",
    "\n",
    "mask_filename = f'{args.mask_type}_n{args.N}_s{args.resolution}_p{args.p1}_{args.input_size[0]}x{args.input_size[1]}'\n",
    "mask_path = args.maskdir + mask_filename + '.npy'\n",
    "print(mask_path)\n",
    "\n",
    "if generate_new or not os.path.isfile(mask_path):\n",
    "    explainer.generate_masks_rise(N=args.N, s=args.resolution, p1=args.p1, savepath= mask_path)\n",
    "else:\n",
    "    explainer.load_masks(mask_path)\n",
    "    print('Masks are loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize 3 generated masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_masks=3\n",
    "masks = explainer.masks[:3]\n",
    "\n",
    "masked_image = torch.mul(masks.to(args.device), tensor)\n",
    "masked_image = masked_image.cpu().numpy()\n",
    "print(masked_image.shape)\n",
    "\n",
    "masks = masks.cpu()\n",
    "if masks.ndim == 4:  # If the masks have a shape of (N, 1, H, W)\n",
    "    masks = masks[:, 0, :, :]\n",
    "\n",
    "fig, axes = plt.subplots(3, num_masks, figsize=(15, 10))\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    ax.imshow(img_np)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Original image {i + 1}')\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    ax.imshow(masks[i], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Mask {i + 1}')\n",
    "\n",
    "for i, ax in enumerate(axes[2]):\n",
    "    ax.imshow(masked_image[i].transpose(1, 2, 0))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Masked Image {i + 1}')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tensor.permute(0,1,3,2).shape)\n",
    "saliency = explainer(\n",
    "    x=tensor,\n",
    "    target_class_indices=args.target_classes,\n",
    "    target_bbox=target_bbox\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot heatmap/saliency map given by XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_saliency_and_targetbb_on_image(\n",
    "        height=args.input_size[0], width=args.input_size[1], \n",
    "        img_name=args.img_name, \n",
    "        img=img_np,\n",
    "        saliency_map=saliency[target_class], \n",
    "        target_class_id= target_class,\n",
    "        target_bbox=target_bbox,\n",
    "        save_to=f'output/{args.img_name}_saliency_targetbb_class{target_class}_{date_time}.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do the same for another target bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes, _ = load_and_convert_bboxes(labels,img_height=args.input_size[0],img_width=args.input_size[1], target_class= target_class)\n",
    "target_bbox = bboxes[1] # select the first bbox --> multiple might be given in the same image\n",
    "plot_image_with_bboxes(img_np,[target_bbox], save_to=f'output/{args.img_name}_predicted_bbox_{date_time}.png')\n",
    "print('Target bbox:',target_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency = explainer(x=tensor,\n",
    "                     target_class_indices=[target_class],\n",
    "                     target_bbox=target_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_bbox_and_saliency = plot_saliency_and_targetbb_on_image(\n",
    "        height=args.input_size[0], width=args.input_size[1], \n",
    "        img_name=args.img_name, \n",
    "        img=img_np,\n",
    "        saliency_map=saliency[target_class], \n",
    "        target_class_id= target_class,\n",
    "        target_bbox=target_bbox\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "#yolo detected bboxes\n",
    "yoloPredictedBboxes = Image.open(f'output/{args.img_name}_predicted_bbox_{date_time}.png')\n",
    "\n",
    "# saliency map with target bbox\n",
    "driseSaliency = Image.open(f'output/{args.img_name}_saliency_targetbb_class{target_class}_{date_time}.png')\n",
    "\n",
    "\n",
    "images = [resized_img, yoloPredictedBboxes, driseSaliency]\n",
    "widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "total_width = sum(widths)\n",
    "max_height = max(heights)\n",
    "\n",
    "composed = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "x_offset = 0\n",
    "for im in images:\n",
    "  composed.paste(im, (x_offset,0))\n",
    "  x_offset += im.size[0]\n",
    "\n",
    "composed.save(f'output/composedImage{target_class}_{date_time}.jpg')\n",
    "\n",
    "\n",
    "instruction = \"\"\"You are the Visual LLM specializing in detailed explanation chaining for Visual-LLMs.\n",
    "You are provided with an image, the bounding box predicted by YOLO and a saliency map for one bounding box generated with DRISE.\n",
    "Give a detailed analysis on Color, Shape and Result to explain how the model reached the bounding box.\n",
    "\n",
    "Do not make up any information that is not present in the image, bounding boxes or saliency map.\n",
    "Do not repeat the same information in different sections.\n",
    "Do not explain any of the used models or concepts. Only focus on the given image, bounding boxes and saliency map.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    composed,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 400,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
